# Chapter 3
Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn


## Choosing a classification algorithm  
1. íŠ¹ì§• ì„ íƒ ë° ë ˆì´ë¸”ë§ëœ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘  
2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì„ íƒ  
3. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë° ëª¨ë¸ êµìœ¡  
4. ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€  
5. ì•Œê³ ë¦¬ì¦˜ì˜ ì„¤ì • ë³€ê²½ ë° ëª¨ë¸ íŠœë‹  
## First steps with scikit-learn â€“ training a perceptron  
## Modeling class probabilities via logistic regression  
#### Logistic regression and conditional probabilities  
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896820-ccf2b591-c8b5-48c7-bc82-176092a4d640.png)
>    
> ë¡œì§“ í•¨ìˆ˜ëŠ” 0 ~ 1 ë²”ìœ„ì˜ ìž…ë ¥ ê°’ì„ ê°€ì ¸ì™€ì„œ ì „ì²´ ì‹¤ìˆ˜ ë²”ìœ„ì— ê±¸ì³ ê°’ìœ¼ë¡œ ë³€í™˜
> $$logit(p) = log\frac{p}{1-p}$$
>   
> ì•„ëž˜ëŠ” ë¡œì§“ í•¨ìˆ˜ì™€ ì„ í˜• ìž…ë ¥ê°„ì˜ ê´€ê³„ê°€ ìžˆë‹¤ê³  ê°€ì •í•œ ìˆ˜ì‹
> $$logit(p) = w_1x_1 + w_2x_2 + \cdots + w_mx_m + b = w^Tx + b$$
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896834-31833ce8-95bb-4ef9-8844-3b435711b922.png)  
>  
>  $$ðœŽ(ð‘§)=\frac{1}{1+e^-z}$$
>  
>  $$z = w^Tx + b$$
>  
#### Learning the model weights via the logistic loss function  
#### Converting an Adaline implementation into an algorithm for logistic regression  
#### Training a logistic regression model with scikit-learn  
#### Tackling overfitting via regularization  
## Maximum margin classification with support vector machines  
#### Maximum margin intuition  
#### Dealing with a nonlinearly separable case using slack variables  
#### Alternative implementations in scikit-learn  
## Solving nonlinear problems using a kernel SVM  
#### Kernel methods for linearly inseparable data  
#### Using the kernel trick to find separating hyperplanes in a high-dimensional space  
## Decision tree learning  
#### Maximizing IG â€“ getting the most bang for your buck  
#### Building a decision tree  
#### Combining multiple decision trees via random forests  
## K-nearest neighbors â€“ a lazy learning algorithm  
## Summary 
 
