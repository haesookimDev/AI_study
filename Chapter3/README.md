# Chapter 3
Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn


## Choosing a classification algorithm  
1. íŠ¹ì§• ì„ íƒ ë° ë ˆì´ë¸”ë§ëœ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘  
2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì„ íƒ  
3. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë° ëª¨ë¸ êµìœ¡  
4. ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€  
5. ì•Œê³ ë¦¬ì¦˜ì˜ ì„¤ì • ë³€ê²½ ë° ëª¨ë¸ íŠœë‹  
## First steps with scikit-learn â€“ training a perceptron  
## Modeling class probabilities via logistic regression  
#### Logistic regression and conditional probabilities  
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896820-ccf2b591-c8b5-48c7-bc82-176092a4d640.png)
>    
> ë¡œì§“ í•¨ìˆ˜ëŠ” 0 ~ 1 ë²”ìœ„ì˜ ì…ë ¥ ê°’ì„ ê°€ì ¸ì™€ì„œ ì „ì²´ ì‹¤ìˆ˜ ë²”ìœ„ì— ê±¸ì³ ê°’ìœ¼ë¡œ ë³€í™˜
> $$logit(p) = log\frac{p}{1-p}$$
>   
> ì•„ë˜ëŠ” ë¡œì§“ í•¨ìˆ˜ì™€ ì„ í˜• ì…ë ¥ê°„ì˜ ê´€ê³„ê°€ ìˆë‹¤ê³  ê°€ì •í•œ ìˆ˜ì‹
> $$logit(p) = w_1x_1 + w_2x_2 + \cdots + w_mx_m + b = w^Tx + b$$
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896834-31833ce8-95bb-4ef9-8844-3b435711b922.png)  
>  
>  $$ğœ(ğ‘§)=\frac{1}{1+e^-z}$$
>  
>  $$z = w^Tx + b$$
>   
#### Learning the model weights via the logistic loss function  
![image](https://user-images.githubusercontent.com/63633387/190898034-922d66c4-d11e-44a8-ab80-b2f3ae97ac8b.png)  
ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ì´ë‹¤. 
![image](https://user-images.githubusercontent.com/63633387/190898058-e7ce0d9d-defe-495f-9f7f-cc0afd6c79d3.png)  
$ğœ(ğ‘§)$ëŠ” ë³€ìˆ˜ $x$ë¥¼ ëª¨ë¸ì´ $y$ë¡œ ì˜ˆì¸¡í•  í™•ë¥ ì´ë‹¤.   
ë¡œê·¸ë¥¼ ì·¨í•˜ëŠ” ì´ìœ ëŠ” ì‚°ìˆ ì—°ì‚°ì˜ ê²°ê³¼ê°€ ì·¨ê¸‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì˜ ë²”ìœ„ ë³´ë‹¤ ì‘ì•„ì§€ëŠ” ìƒíƒœì¸ ì‚°ìˆ  ì–¸ë”í”Œë¡œì˜ ê°€ëŠ¥ì„±ì„ ì¤„ì´ê¸° ìœ„í•¨ì´ë‹¤.
#### Converting an Adaline implementation into an algorithm for logistic regression  
#### Training a logistic regression model with scikit-learn  
#### Tackling overfitting via regularization  
![image](https://user-images.githubusercontent.com/63633387/190898417-16158175-2f38-4af6-b3c7-cf48612ddfa5.png)  
ê³¼ëŒ€ì í•©ì€ í•™ìŠµ ë°ì´í„°ì—ì„œëŠ” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸(ì‹¤ì œ)ë°ì´í„°ì—ì„œ ì¼ë°˜í™”ë˜ì§€ ì•Šì•„ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë§í•œë‹¤.  
í•´ë‹¹ë¬¸ì œëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ í•™ìŠµ í•  ë§Œí¼ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ëª¨ë¸ì˜ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµ í• ë§Œí¼ ë°ì´í„°ì˜ ì§ˆì´ ë‚®ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  
## Maximum margin classification with support vector machines  
![image](https://user-images.githubusercontent.com/63633387/190898627-a644546d-54b3-4cb6-bd91-d7a1b0fd0563.png)  
ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì€ ê²°ì • ê²½ê³„ì™€ ì„œí¬íŠ¸ ë²¡í„° ì‚¬ì´ì˜ ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ìµœì ì˜ ê²°ì • ê²½ê³„ë¥¼ ì°¾ëŠ” ë¶„ë¥˜ëª¨ë¸ì´ë‹¤. 
#### Maximum margin intuition  
#### Dealing with a nonlinearly separable case using slack variables  
![image](https://user-images.githubusercontent.com/63633387/190898826-7d2af00c-56ee-437a-a540-10ae916c1893.png)  
C íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ì˜¤ë¶„ë¥˜ì— ëŒ€í•œ íŒ¨ë„í‹°ë¥¼ ì œì–´í•œë‹¤. CíŒŒë¼ë¯¸í„°ê°€ í¬ë©´ ì˜¤ë¶„ë¥˜ë¥¼ ì—„ê²©í•˜ê²Œ ê´€ë¦¬í•˜ì§€ë§Œ ê³¼ëŒ€ì í•©ì˜ ìœ„í—˜ì´ ìˆì„ ìˆ˜ ìˆë‹¤.

#### Alternative implementations in scikit-learn  
## Solving nonlinear problems using a kernel SVM  
ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì€ ë¹„ì„ í˜• ë°ì´í„°ì—ì„œë„ ë¶„ë¥˜ë¥¼ ê°€ëŠ¥í•˜ê²Œí•œë‹¤.
#### Kernel methods for linearly inseparable data  
#### Using the kernel trick to find separating hyperplanes in a high-dimensional space  
ì„œí¬íŠ¸ ë²¡í„°ë¨¸ì‹ ì€ ë¹„ì„ í˜•ë°ì´í„°ë¥¼ ê³ ì°¨ì› íŠ¹ì§•ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë‚˜íƒ€ë‚œ ë°ì´í„° ì°¨ì›ì—ì„œ ì„ í˜• í‰ë©´ì„ ë§Œë“¤ì–´ ë¶„ë¥˜í•œë‹¤.  
![image](https://user-images.githubusercontent.com/63633387/190899073-398374da-4e48-4b56-8b2f-dfbc2c058cdb.png)  
![image](https://user-images.githubusercontent.com/63633387/190899087-d4120a30-d0fb-4fce-8916-f81d795a5d82.png)  
  
  ìœ„ì²˜ëŸ¼ ì €ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ê³ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ì¼ì´ë‹¤.   
  ë”°ë¼ì„œ ì»¤ë„ íŠ¸ë¦­ì´ë¼ëŠ” ê²ƒì„ ì´ìš©í•œë‹¤.  
  ![image](https://user-images.githubusercontent.com/63633387/190899396-37fdd0f2-2e36-4dff-9813-077e53c269bc.png)  
  ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì»¤ë„ì€ ê°€ìš°ì‹œì•ˆ ì»¤ë„(í•œìŒì˜ íŠ¹ì§•ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ìœ ì‚¬ë„ í•¨ìˆ˜)ì´ë¼ëŠ” ê²ƒì„ ì‚¬ìš©í•œë‹¤.   
  ![image](https://user-images.githubusercontent.com/63633387/190899520-103c7e19-6dd2-4c2a-bd99-c41c1cf952d6.png)  
  ì§€ìˆ˜í•­ì„ í†µí•´ ê±°ë¦¬ê°’ì˜ ë²”ìœ„ëŠ” 0(ìœ ì‚¬ë„ ë‚®ìŒ)ê³¼ 1(ìœ ì‚¬ë„ ë†’ìŒ)ì‚¬ì´ê°€ ëœë‹¤.

## Decision tree learning  
#### Maximizing IG â€“ getting the most bang for your buck  
#### Building a decision tree  
#### Combining multiple decision trees via random forests  
## K-nearest neighbors â€“ a lazy learning algorithm  
## Summary 
 
