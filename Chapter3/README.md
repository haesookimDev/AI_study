# Chapter 3
Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn


## Choosing a classification algorithm  
1. íŠ¹ì§• ì„ íƒ ë° ë ˆì´ë¸”ë§ëœ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘  
2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì„ íƒ  
3. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë° ëª¨ë¸ êµìœ¡  
4. ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€  
5. ì•Œê³ ë¦¬ì¦˜ì˜ ì„¤ì • ë³€ê²½ ë° ëª¨ë¸ íŠœë‹  
## First steps with scikit-learn â€“ training a perceptron  
## Modeling class probabilities via logistic regression  
#### Logistic regression and conditional probabilities  
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896820-ccf2b591-c8b5-48c7-bc82-176092a4d640.png)
>    
> ë¡œì§“ í•¨ìˆ˜ëŠ” 0 ~ 1 ë²”ìœ„ì˜ ì…ë ¥ ê°’ì„ ê°€ì ¸ì™€ì„œ ì „ì²´ ì‹¤ìˆ˜ ë²”ìœ„ì— ê±¸ì³ ê°’ìœ¼ë¡œ ë³€í™˜
> $$logit(p) = log\frac{p}{1-p}$$
>   
> ì•„ë˜ëŠ” ë¡œì§“ í•¨ìˆ˜ì™€ ì„ í˜• ì…ë ¥ê°„ì˜ ê´€ê³„ê°€ ìˆë‹¤ê³  ê°€ì •í•œ ìˆ˜ì‹
> $$logit(p) = w_1x_1 + w_2x_2 + \cdots + w_mx_m + b = w^Tx + b$$
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896834-31833ce8-95bb-4ef9-8844-3b435711b922.png)  
>  
>  $$ğœ(ğ‘§)=\frac{1}{1+e^-z}$$
>  
>  $$z = w^Tx + b$$
>   
#### Learning the model weights via the logistic loss function  
![image](https://user-images.githubusercontent.com/63633387/190898034-922d66c4-d11e-44a8-ab80-b2f3ae97ac8b.png)  
ë¡œì§“í•¨ìˆ˜ë¥¼ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ í†µí•´ ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •ë²•ì„ ê³„ì‚°í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ì´ë‹¤. 
![image](https://user-images.githubusercontent.com/63633387/190898058-e7ce0d9d-defe-495f-9f7f-cc0afd6c79d3.png)  
$ğœ(ğ‘§)$ëŠ” ë³€ìˆ˜ $x$ë¥¼ ëª¨ë¸ì´ $y$ë¡œ ì˜ˆì¸¡í•  í™•ë¥ ì´ë‹¤.   
ë¡œê·¸ë¥¼ ì·¨í•˜ëŠ” ì´ìœ ëŠ” ì‚°ìˆ ì—°ì‚°ì˜ ê²°ê³¼ê°€ ì·¨ê¸‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì˜ ë²”ìœ„ ë³´ë‹¤ ì‘ì•„ì§€ëŠ” ìƒíƒœì¸ ì‚°ìˆ  ì–¸ë”í”Œë¡œì˜ ê°€ëŠ¥ì„±ì„ ì¤„ì´ê¸° ìœ„í•¨ì´ë‹¤.
#### Converting an Adaline implementation into an algorithm for logistic regression  
#### Training a logistic regression model with scikit-learn  
#### Tackling overfitting via regularization  
![image](https://user-images.githubusercontent.com/63633387/190898417-16158175-2f38-4af6-b3c7-cf48612ddfa5.png)  
ê³¼ëŒ€ì í•©ì€ í•™ìŠµ ë°ì´í„°ì—ì„œëŠ” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸(ì‹¤ì œ)ë°ì´í„°ì—ì„œ ì¼ë°˜í™”ë˜ì§€ ì•Šì•„ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë§í•œë‹¤.  
í•´ë‹¹ë¬¸ì œëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ í•™ìŠµ í•  ë§Œí¼ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ëª¨ë¸ì˜ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµ í• ë§Œí¼ ë°ì´í„°ì˜ ì§ˆì´ ë‚®ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  
## Maximum margin classification with support vector machines  
![image](https://user-images.githubusercontent.com/63633387/190898627-a644546d-54b3-4cb6-bd91-d7a1b0fd0563.png)  
ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì€ ê²°ì • ê²½ê³„ì™€ ì„œí¬íŠ¸ ë²¡í„° ì‚¬ì´ì˜ ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ìµœì ì˜ ê²°ì • ê²½ê³„ë¥¼ ì°¾ëŠ” ë¶„ë¥˜ëª¨ë¸ì´ë‹¤. 
#### Maximum margin intuition  
#### Dealing with a nonlinearly separable case using slack variables  
![image](https://user-images.githubusercontent.com/63633387/190898826-7d2af00c-56ee-437a-a540-10ae916c1893.png)  
C íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ì˜¤ë¶„ë¥˜ì— ëŒ€í•œ íŒ¨ë„í‹°ë¥¼ ì œì–´í•œë‹¤. CíŒŒë¼ë¯¸í„°ê°€ í¬ë©´ ì˜¤ë¶„ë¥˜ë¥¼ ì—„ê²©í•˜ê²Œ ê´€ë¦¬í•˜ì§€ë§Œ ê³¼ëŒ€ì í•©ì˜ ìœ„í—˜ì´ ìˆì„ ìˆ˜ ìˆë‹¤.

#### Alternative implementations in scikit-learn  
## Solving nonlinear problems using a kernel SVM  
ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì€ ë¹„ì„ í˜• ë°ì´í„°ì—ì„œë„ ë¶„ë¥˜ë¥¼ ê°€ëŠ¥í•˜ê²Œí•œë‹¤.
#### Kernel methods for linearly inseparable data  
#### Using the kernel trick to find separating hyperplanes in a high-dimensional space  
ì„œí¬íŠ¸ ë²¡í„°ë¨¸ì‹ ì€ ë¹„ì„ í˜•ë°ì´í„°ë¥¼ ê³ ì°¨ì› íŠ¹ì§•ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ë‚˜íƒ€ë‚œ ë°ì´í„° ì°¨ì›ì—ì„œ ì„ í˜• í‰ë©´ì„ ë§Œë“¤ì–´ ë¶„ë¥˜í•œë‹¤.  
![image](https://user-images.githubusercontent.com/63633387/190899073-398374da-4e48-4b56-8b2f-dfbc2c058cdb.png)  
![image](https://user-images.githubusercontent.com/63633387/190899087-d4120a30-d0fb-4fce-8916-f81d795a5d82.png)  
  
  ìœ„ì²˜ëŸ¼ ì €ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ê³ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ì¼ì´ë‹¤.   
  ë”°ë¼ì„œ ì»¤ë„ íŠ¸ë¦­ì´ë¼ëŠ” ê²ƒì„ ì´ìš©í•œë‹¤.  
  ![image](https://user-images.githubusercontent.com/63633387/190899396-37fdd0f2-2e36-4dff-9813-077e53c269bc.png)  
  ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì»¤ë„ì€ ê°€ìš°ì‹œì•ˆ ì»¤ë„(í•œìŒì˜ íŠ¹ì§•ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ìœ ì‚¬ë„ í•¨ìˆ˜)ì´ë¼ëŠ” ê²ƒì„ ì‚¬ìš©í•œë‹¤.   
  ![image](https://user-images.githubusercontent.com/63633387/190899520-103c7e19-6dd2-4c2a-bd99-c41c1cf952d6.png)  
  ì§€ìˆ˜í•­ì„ í†µí•´ ê±°ë¦¬ê°’ì˜ ë²”ìœ„ëŠ” 0(ìœ ì‚¬ë„ ë‚®ìŒ)ê³¼ 1(ìœ ì‚¬ë„ ë†’ìŒ)ì‚¬ì´ê°€ ëœë‹¤.

## Decision tree learning  
  ![image](https://user-images.githubusercontent.com/63633387/190899836-e866b4a6-d43b-4a9b-b1c7-b4b531fe82e3.png)
  ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜ì˜ ê°€ì§€ì²˜ëŸ¼ ë»—ì–´ë‚˜ê°€ë©° ê²°ê³¼ë¥¼ ê²°ì •í•˜ëŠ” í•™ìŠµ ëª¨ë¸ì´ë‹¤.
  ë„ì¶œëœ ê²°ê³¼ë¥¼ í•´ì„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ê³¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.
  ë…¸ë“œê°€ ë§ì•„ì§€ê³  ê¹Šì´ê°€ ê¹Šì–´ì§€ë©´ í•™ìŠµê³¼ì •ì—ì„œëŠ” ë†’ì€ ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ 
  ì‹¤ì œ ë°ì´í„°ë¥¼ ì´ìš©í•  ë•Œ ì¼ë°˜í™”ê°€ ë˜ì§€ì•ŠëŠ” ê³¼ëŒ€ì í•©ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤.

#### Maximizing IG â€“ getting the most bang for your buck  
  ![image](https://user-images.githubusercontent.com/63633387/190900069-32050695-bec1-4e73-8011-0e398dbe33ae.png)  
  ìœ„ì˜ í•¨ìˆ˜ëŠ” ê²°ì •íŠ¸ë¦¬ì—ì„œ ìµœì í™”ì— ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.  
  $D$ëŠ” ë°ì´í„° $N$ì€ ë…¸ë“œì—ì„œ í•™ìŠµë˜ëŠ” $x$ì˜ ìˆ˜ ì´ë©° $I$ëŠ” impurity ì¸¡ì •ë²•ì´ë©° $p$ëŠ” ë¶€ëª¨, $j$ëŠ” ìì‹ì´ë‹¤.  
  ì •ë³´ì´ë“ì„ ìµœëŒ€í™” í•˜ëŠ” ë°©ë²•ì€ ìì‹ë…¸ë“œì—ì„œì˜ impurityê°€ ë‚®ì„ ìˆ˜ë¡ ë†’ë‹¤
  ![image](https://user-images.githubusercontent.com/63633387/190900159-6442941d-59a8-4b52-95a6-ba01f70dcd31.png)  
  ìœ„ëŠ” ëŒ€ë¶€ë¶„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ êµ¬í˜„ë˜ëŠ” ì´ì§„ ê²°ì •íŠ¸ë¦¬ì˜ ì •ë³´ì´ë“ ê³„ì‚°ì‹ì´ë‹¤.
  
  ì´ì§„ ê²°ì •íŠ¸ë¦¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¶„ë¥˜ ê¸°ì¤€ì€ Gini impurity $(I_G)$, entropy $(I_H)$ classification error $(I_E)$ì´ë‹¤.
  
  ![image](https://user-images.githubusercontent.com/63633387/190900352-c73184a8-e5e9-4153-a330-254a309dda85.png)  
  ![image](https://user-images.githubusercontent.com/63633387/190900380-2a0b5b20-1ef2-47d8-ad4d-022b8c568d44.png)  
  ![image](https://user-images.githubusercontent.com/63633387/190900391-7a2d3a34-00af-40dc-a578-058f0a1098cb.png)  
  
  $p(i|t)$ëŠ” íŠ¹ì • ë…¸ë“œ, $t$ì— ëŒ€í•œ í´ë˜ìŠ¤ $i$ì— ì†í•˜ëŠ” $x$ë“¤ì˜ ë¹„ìœ¨ì´ë‹¤.
  
#### Building a decision tree  
#### Combining multiple decision trees via random forests  
  ì—¬ëŸ¬ ê²°ì •íŠ¸ë¦¬ì˜ ê²°ê³¼ë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ ê²°ë¡ ì„ ë‚´ëŠ” ê²ƒì„ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¼ ë¶€ë¥´ë©° ëª¨ë¸ì˜ ì•™ìƒë¸”ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.  
  ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤. 
  1. í¬ê¸°ê°€ nì¸ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ì¶”ì¶œí•œë‹¤(í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë¬´ì‘ìœ„ë¡œ nê°œì˜ $x$ë¥¼ ì„ íƒí•œë‹¤).  
  2. ìƒ˜í”Œì—ì„œ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµí•œë‹¤.
    ê° ë…¸ë“œ:  
    a. êµì²´í•˜ì§€ ì•Šê³  ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œ íŠ¹ì§•
    b. ëª©ì  í•¨ìˆ˜ì— ë”°ë¼ ìµœì ì˜ ë¶„í• ì„ ì œê³µí•˜ëŠ” ê¸°ëŠ¥, ì˜ˆë¥¼ ë“¤ì–´ ì •ë³´ ì´ë“ì„ ìµœëŒ€í™”í•˜ëŠ” ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë…¸ë“œë¥¼ ë¶„í•   
  3. 1 ~ 2 kíšŒ ë‹¨ê³„ë¥¼ ë°˜ë³µí•œë‹¤.  
  4. ê° íŠ¸ë¦¬ë³„ ì˜ˆì¸¡ì„ ì§‘ê³„í•˜ì—¬ ë‹¤ìˆ˜ê²°ë¡œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ í• ë‹¹í•œë‹¤.

## K-nearest neighbors â€“ a lazy learning algorithm  
  ë‹¨ìˆœí•¨ ë•Œë¬¸ì´ ì•„ë‹ˆë¼ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì°¨ë³„ì  ê¸°ëŠ¥ì„ í•™ìŠµí•˜ì§€ ì•Šê³  í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ ëŒ€ì‹  ì™¸ìš°ê¸° ë•Œë¬¸ì— 'a lazy learning algorithm'ìœ¼ë¡œ ë¶ˆë¦°ë‹¤.  
  
  KNN ì•Œê³ ë¦¬ì¦˜ ìì²´ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë©° ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ìš”ì•½í•  ìˆ˜ ìˆë‹¤.  
  1. kì˜ ìˆ˜ì™€ ê±°ë¦¬ ë©”íŠ¸ë¦­ì„ ì„ íƒí•©ë‹ˆë‹¤.  
  2. ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” ë°ì´í„° ë ˆì½”ë“œì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ì°¾ê¸°  
  3. ë‹¤ìˆ˜ê²°ë¡œ í´ë˜ìŠ¤ë¥¼ ì§€ì •í•œë‹¤.  
  
  ![image](https://user-images.githubusercontent.com/63633387/190900812-f98d7448-36bb-4df6-9451-9722303d3950.png)  
  
  KNN ì•Œê³ ë¦¬ì¦˜ì€ ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” ì§€ì ì— ê°€ì¥ ê°€ê¹Œìš´(ê°€ì¥ ìœ ì‚¬í•œ) í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì—ì„œ kê°œì˜ ì˜ˆë¥¼ ì°¾ëŠ”ë‹¤.  
  ë°ì´í„° í¬ì¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¼ë²¨ì€ kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒë“¤ ì‚¬ì´ì˜ ë‹¤ìˆ˜ê²°ì— ì˜í•´ ê²°ì •

## Summary 
 
