# Chapter 3
Chapter 3: A Tour of Machine Learning Classifiers Using Scikit-Learn


## Choosing a classification algorithm  
1. íŠ¹ì§• ì„ íƒ ë° ë ˆì´ë¸”ë§ëœ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘  
2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì„ íƒ  
3. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë° ëª¨ë¸ êµìœ¡  
4. ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€  
5. ì•Œê³ ë¦¬ì¦˜ì˜ ì„¤ì • ë³€ê²½ ë° ëª¨ë¸ íŠœë‹  
## First steps with scikit-learn â€“ training a perceptron  
## Modeling class probabilities via logistic regression  
#### Logistic regression and conditional probabilities  
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896820-ccf2b591-c8b5-48c7-bc82-176092a4d640.png)
>    
> ë¡œì§“ í•¨ìˆ˜ëŠ” 0 ~ 1 ë²”ìœ„ì˜ ì…ë ¥ ê°’ì„ ê°€ì ¸ì™€ì„œ ì „ì²´ ì‹¤ìˆ˜ ë²”ìœ„ì— ê±¸ì³ ê°’ìœ¼ë¡œ ë³€í™˜
> $$logit(p) = log\frac{p}{1-p}$$
>   
> ì•„ë˜ëŠ” ë¡œì§“ í•¨ìˆ˜ì™€ ì„ í˜• ì…ë ¥ê°„ì˜ ê´€ê³„ê°€ ìˆë‹¤ê³  ê°€ì •í•œ ìˆ˜ì‹
> $$logit(p) = w_1x_1 + w_2x_2 + \cdots + w_mx_m + b = w^Tx + b$$
>   
> ![image](https://user-images.githubusercontent.com/63633387/190896834-31833ce8-95bb-4ef9-8844-3b435711b922.png)  
>  
>  $$ğœ(ğ‘§)=\frac{1}{1+e^-z}$$
>  
>  $$z = w^Tx + b$$
>   
#### Learning the model weights via the logistic loss function  
![image](https://user-images.githubusercontent.com/63633387/190898034-922d66c4-d11e-44a8-ab80-b2f3ae97ac8b.png)
ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ì´ë‹¤. 
![image](https://user-images.githubusercontent.com/63633387/190898058-e7ce0d9d-defe-495f-9f7f-cc0afd6c79d3.png)
$ğœ(ğ‘§)$ëŠ” ë³€ìˆ˜ $x$ë¥¼ ëª¨ë¸ì´ $y$ë¡œ ì˜ˆì¸¡í•  í™•ë¥ ì´ë‹¤.   
ë¡œê·¸ë¥¼ ì·¨í•˜ëŠ” ì´ìœ ëŠ” ì‚°ìˆ ì—°ì‚°ì˜ ê²°ê³¼ê°€ ì·¨ê¸‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì˜ ë²”ìœ„ ë³´ë‹¤ ì‘ì•„ì§€ëŠ” ìƒíƒœì¸ ì‚°ìˆ  ì–¸ë”í”Œë¡œì˜ ê°€ëŠ¥ì„±ì„ ì¤„ì´ê¸° ìœ„í•¨ì´ë‹¤.
#### Converting an Adaline implementation into an algorithm for logistic regression  
#### Training a logistic regression model with scikit-learn  
#### Tackling overfitting via regularization  
## Maximum margin classification with support vector machines  
#### Maximum margin intuition  
#### Dealing with a nonlinearly separable case using slack variables  
#### Alternative implementations in scikit-learn  
## Solving nonlinear problems using a kernel SVM  
#### Kernel methods for linearly inseparable data  
#### Using the kernel trick to find separating hyperplanes in a high-dimensional space  
## Decision tree learning  
#### Maximizing IG â€“ getting the most bang for your buck  
#### Building a decision tree  
#### Combining multiple decision trees via random forests  
## K-nearest neighbors â€“ a lazy learning algorithm  
## Summary 
 
